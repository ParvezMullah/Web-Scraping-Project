{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from collections import OrderedDict\n",
    "from urllib.request import urlopen\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts_links = []\n",
    "top_posts_details = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_links(tag):\n",
    "    post_links = OrderedDict()\n",
    "    url = 'https://medium.com/tag/' + tag\n",
    "    source = requests.get(url).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    container = soup.find('div', class_ ='js-tagStream')\n",
    "    i = 0\n",
    "    for div in container:\n",
    "        anchor_tag = div.find('a', class_='link link--darken')\n",
    "        post_link = anchor_tag.get('data-action-value')\n",
    "        post_author = (div.find('a', class_='ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken')).text\n",
    "        read = (div.find('span', class_='readingTime')).get('title')\n",
    "        details = div.find('time').text +', ' + str(read)\n",
    "        post_links[post_link] = [post_author, details]\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break\n",
    "    return post_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts_links = get_top_links('Blockchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_details(link, author):\n",
    "    details = {}\n",
    "    source = requests.get(link).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    details['author_name'] = author[0]\n",
    "    details['title'] = (soup.find('h1')).text\n",
    "    details['details'] = author[1]\n",
    "    details['blog'] = soup.find('div', class_='section-inner sectionLayout--insetColumn')\n",
    "    (soup.find('h1')).decompose()\n",
    "    tags = soup.findAll('a', class_='link u-baseColor--link')\n",
    "    details['tags'] = [a.text for a in tags]\n",
    "    try:\n",
    "        details['responses'] = (soup.find('button', class_='button button--chromeless u-baseColor--buttonNormal')).text\n",
    "    except:\n",
    "        details['responses'] = 0\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_post_details(top_posts_links):\n",
    "    post_details = OrderedDict()\n",
    "    for link in top_posts_links.keys():\n",
    "        post_details[link] = get_post_details(link, top_posts_links[link])\n",
    "    return post_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts_details = get_all_post_details(top_posts_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in top_posts_details.keys():\n",
    "#     print(top_posts_details[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from collections import OrderedDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-122-d0bddd6fc5cc>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-122-d0bddd6fc5cc>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    source = requests.get(url).text\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "container = None\n",
    "\n",
    "def get_top_links(tag):\n",
    "    global top_posts_list\n",
    "    global container\n",
    "\n",
    "    if container is None:\n",
    "        url = 'https://medium.com/tag/' + tag\n",
    "         source = requests.get(url).text\n",
    "#         driver = webdriver.PhantomJS('C:\\\\Users\\\\parvez\\\\Desktop\\\\Scraping Project\\\\mediumscrapingapp\\\\phantomjs')\n",
    "#         # Load Twitter page and click to view all results\n",
    "#         driver.get(url)\n",
    "        soup = BeautifulSoup(source, 'html5lib')\n",
    "        container = soup.findAll('div', class_ ='streamItem streamItem--postPreview js-streamItem')\n",
    "\n",
    "    div = container[0]\n",
    "    container.pop(0)\n",
    "    top_posts = {}\n",
    "    anchor_tag = div.find('a', class_='link link--darken')\n",
    "    post_link = (anchor_tag.get('data-action-value')).split('?source')[0]\n",
    "    post_title = div.find('h3', class_='graf graf--h3 graf-after--figure graf--trailing graf--title')\n",
    "    if post_title == None:\n",
    "        post_title = div.find('h3', class_=\"graf graf--h3 graf-after--figure graf--title\")\n",
    "    nonBreakSpace = u'\\xa0'\n",
    "    if post_title is None:\n",
    "        post_title = \"not found\"\n",
    "    post_title = (post_title.text).replace(nonBreakSpace, ' ')\n",
    "    post_author = (div.find('a', class_='ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken')).text\n",
    "    post_read = (div.find('span', class_='readingTime')).get('title')\n",
    "    post_details = div.find('time').text +', ' + str(post_read)\n",
    "    top_posts['post_link'] = post_link\n",
    "    top_posts['post_title'] = post_title\n",
    "    top_posts['post_author'] = post_author\n",
    "    top_posts['post_details'] = post_details\n",
    "    return top_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"https://medium.com/tag/programming\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-971432e98b44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_top_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'programming'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-e34ea1720b9f>\u001b[0m in \u001b[0;36mget_top_links\u001b[1;34m(tag)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'streamItem streamItem--postPreview js-streamItem'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtop_posts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    data = get_top_links('programming')\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from selenium import webdriver\n",
    "\n",
    "def get_post_details(url):\n",
    "    post_detail_view = {}\n",
    "    # source = requests.get(post_link).text\n",
    "    source = url.split('details/')[-1]\n",
    "    #post_title = (source.split('/')[-1]).split('-')[:-1]\n",
    "    #post_detail_view['title'] = ' '.join(post_title)  \n",
    "    post_detail_view['source_url'] = source  \n",
    "    \n",
    "    driver = webdriver.PhantomJS('C:\\\\Users\\\\parvez\\\\Desktop\\\\Scraping Project\\\\mediumscrapingapp\\\\phantomjs')\n",
    "    url = 'https://medium.com/aerum-technologies/wish-id-bought-bitcoin-in-13-af7d9f580853'\n",
    "    SCROLL_PAUSE_TIME = 1\n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "\n",
    "    post_detail_view['details'] = soup.find('span',attrs={'class':'readingTime'})['title']\n",
    "    post_detail_view['blog'] = (soup.find('div', class_='section-inner sectionLayout--insetColumn')).text\n",
    "    post_detail_view['title'] = (soup.find('h1', class_='graf graf--h3 graf--leading graf--title')).text\n",
    "    # (soup.find('h1')).decompose()\n",
    "    tags = soup.findAll('a', class_='link u-baseColor--link')\n",
    "    post_detail_view['tags'] = [a.text for a in tags]\n",
    "    return post_detail_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1a893258f546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://127.0.0.1:8000/details/https://medium.com/@brideinreverse/why-cooking-for-one-sucks-and-how-to-make-it-less-lonely-a41b72f9a69d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://medium.com/aerum-technologies/blockchain-use-cases-more-than-just-potential-9b8b335b14b9'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mpost_detail_view\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_post_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;31m# del post_detail_view['blog']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# print(soup.findAll('div', class_='streamItem streamItem--postPreview js-streamItem'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-1a893258f546>\u001b[0m in \u001b[0;36mget_post_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'link u-baseColor--link'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mpost_detail_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mbtn_click\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'button'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto is-touched'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbtn_click\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_post_details(url):\n",
    "    post_detail_view = {}\n",
    "    # source = requests.get(post_link).text\n",
    "    source = url.split('details/')[-1]\n",
    "    #post_title = (source.split('/')[-1]).split('-')[:-1]\n",
    "    #post_detail_view['title'] = ' '.join(post_title)  \n",
    "    post_detail_view['source_url'] = source  \n",
    "    \n",
    "    driver = webdriver.PhantomJS('C:\\\\Users\\\\parvez\\\\Desktop\\\\Scraping Project\\\\mediumscrapingapp\\\\phantomjs')\n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "    btn_click = driver.find_element_by_xpath(\"//button[@class='button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto']\")\n",
    "    #btn_cls = 'button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto is-touched'\n",
    "    #response_button = soup.find('button', class_=btn_cls)\n",
    "#     #response_button = soup.find('div', class_='container js-showOtherResponses')\n",
    "#     response_button.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "    post_detail_view['details'] = soup.find('span',attrs={'class':'readingTime'})['title']\n",
    "    post_detail_view['blog'] = (soup.find('div', class_='section-inner sectionLayout--insetColumn')).text\n",
    "    post_detail_view['title'] = (soup.find('h1', class_='graf graf--h3 graf--leading graf--title')).text\n",
    "    post_detail_view['responses'] = soup.findAll('p', class_='graf graf--p graf--leading graf--trailing')\n",
    "    tags = soup.findAll('a', class_='link u-baseColor--link')\n",
    "    post_detail_view['tags'] = [a.text for a in tags]\n",
    "    btn_click = soup.find('button', class_='button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto is-touched').text\n",
    "\n",
    "    return btn_click\n",
    "\n",
    "url = 'http://127.0.0.1:8000/details/https://medium.com/@brideinreverse/why-cooking-for-one-sucks-and-how-to-make-it-less-lonely-a41b72f9a69d'\n",
    "url = 'https://medium.com/aerum-technologies/blockchain-use-cases-more-than-just-potential-9b8b335b14b9'\n",
    "post_detail_view = get_post_details(url)\n",
    "# del post_detail_view['blog']\n",
    "# print(soup.findAll('div', class_='streamItem streamItem--postPreview js-streamItem'))\n",
    "print(post_detail_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "driver = webdriver.PhantomJS('C:\\\\Users\\\\parvez\\\\Desktop\\\\Scraping Project\\\\mediumscrapingapp\\\\phantomjs')\n",
    "url = 'https://medium.com/aerum-technologies/wish-id-bought-bitcoin-in-13-af7d9f580853'\n",
    "SCROLL_PAUSE_TIME = 1\n",
    "driver.get(url)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "details = soup.find('div', class_='u-flex1 u-paddingLeft15 u-overflowHidden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('span',attrs={'class':'readingTime'})['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto is-touched' == 'button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto is-touched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'graf graf--p graf--leading graf--trailing' == 'graf graf--p graf--leading graf--trailing'\n",
    "button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto is-touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\parvez\\\\Desktop\\\\Scraping Project\\\\phantomjs'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "full_path_of_phantomjs = str(os.getcwd()) + \"\\\\phantomjs\"\n",
    "full_path_of_phantomjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'C:\\\\Users\\\\parvez\\\\Desktop\\\\Scraping Project\\\\mediumscrapingapp\\\\phantomjs'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
